<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="mystyle.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css">

    <title>Code4TOM</title>
  </head>

  <body>
    <div class="landing-wrapper">
    <div class="header">
      <div style="background-image:linear-gradient(to right, #7f00ff, #e100ff);text-align:center; height:200px; width:100%; padding:10px;">
        <div style=" width=100%; height=100%; margin:0 auto; text-align:center; margin-top:40px;">
          <!--Logo-->
          <svg height="122.66999778747558" width="455.8769230769231" style="width: 455.877px; height: 122.67px; position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%) scale(1); z-index: 0;">
            <g transform="matrix(1.45,0,0,1.45,-18.704996681213377,-11.164998340606688)" fill="#ffffff">
              <path d="M79.4,16.3c-4.3,0-7.8,3.5-7.8,7.8c0,1.5,0.4,2.9,1.2,4.1L62.9,36c-2.5-2.8-6.2-4.6-10.3-4.6c-3.7,0-7,1.5-9.5,3.8  L28.3,21.5c1.1-1.5,1.8-3.2,1.8-5.2c0-4.8-3.9-8.6-8.6-8.6s-8.6,3.9-8.6,8.6s3.9,8.6,8.6,8.6c2.2,0,4.2-0.8,5.7-2.2l14.7,13.7  c-2,2.4-3.1,5.4-3.1,8.8c0,4.9,2.5,9.1,6.3,11.6l-8.9,15.7c-1.3-0.6-2.8-1-4.3-1c-5.7,0-10.4,4.6-10.4,10.4  c0,5.7,4.6,10.4,10.4,10.4S42.2,87.7,42.2,82c0-3.6-1.8-6.7-4.5-8.6l8.9-15.7c1.8,0.9,3.9,1.4,6,1.4c2.9,0,5.6-0.9,7.8-2.4l7.3,9.3  c-0.8,0.9-1.4,2.1-1.4,3.5c0,2.9,2.3,5.2,5.2,5.2c2.9,0,5.2-2.3,5.2-5.2c0-2.9-2.3-5.2-5.2-5.2c-0.9,0-1.7,0.2-2.5,0.6l-7.4-9.4  c2.8-2.5,4.6-6.2,4.6-10.3c0-2.9-0.9-5.6-2.5-7.9l9.9-7.8c1.4,1.4,3.4,2.3,5.5,2.3c4.3,0,7.8-3.5,7.8-7.8S83.7,16.3,79.4,16.3z">
              </path>
            </g>
              <g transform="matrix(3.460963273613639,0,0,3.460963273613639,124.17822385979744,15.01184754096441)" fill="#ffffff">
                <path d="M6.18 7.32 c1.82 0 3.76 0.7 3.76 2.76 c0 0.48 -0.32 0.7 -0.66 0.7 s-0.7 -0.24 -0.7 -0.7 c0 -0.16 -0.1 -1.46 -2.4 -1.46 c-1.82 0 -2.62 0.72 -2.62 2.44 l0 4.82 c0 1.74 0.98 2.28 2.62 2.28 c1.66 0 2.58 -0.68 2.58 -2.3 c0 -0.46 0.34 -0.68 0.66 -0.68 c0.34 0 0.66 0.22 0.66 0.68 c0 2.7 -1.5 3.6 -3.9 3.6 c-2.6 0 -3.92 -0.96 -3.92 -3.58 l0 -4.82 c0 -2.84 1.42 -3.74 3.92 -3.74 z M14.620000000000001 15.86 l0 -4.96 c0 -2.4 1.24 -3.68 3.64 -3.68 l0.52 0 c2.38 0 3.64 1.28 3.64 3.68 l0 4.96 c0 2.2 -1.46 3.64 -3.64 3.64 l-0.52 0 c-2.22 0 -3.64 -1.46 -3.64 -3.64 z M18.26 18.16 l0.52 0 c1.44 0 2.3 -0.9 2.3 -2.3 l0 -4.96 c0 -1.56 -0.76 -2.36 -2.3 -2.36 l-0.52 0 c-1.52 0 -2.36 0.82 -2.36 2.36 l0 4.96 c0 1.46 0.92 2.3 2.36 2.3 z M27.24 18.78 l0 -10.76 c0 -0.54 0.38 -0.66 0.66 -0.66 l3.2 0 c2.64 0 3.94 1.26 3.94 3.74 l0 5.12 c0 1.56 -1.22 3.2 -3.88 3.2 l-3.26 0 c-0.28 0 -0.66 -0.16 -0.66 -0.64 z M33.72 16.22 l0 -5.12 c0 -1.64 -0.88 -2.4 -2.62 -2.4 l-2.54 0 l0 9.42 l2.6 0 c1.22 0 2.56 -0.66 2.56 -1.9 z M39.8 18.7 l0 -10.64 c0 -0.46 0.36 -0.7 0.7 -0.7 l4.6 0 c0.44 0 0.66 0.32 0.66 0.66 s-0.22 0.68 -0.66 0.68 l-3.96 0 l0 3.88 l3.14 0 c0.46 0 0.68 0.32 0.68 0.66 s-0.24 0.7 -0.68 0.7 l-3.14 0 l0 4.14 l3.96 0 c0.42 0 0.64 0.34 0.64 0.66 c0 0.34 -0.22 0.66 -0.64 0.66 l-4.6 0 c-0.34 0 -0.7 -0.12 -0.7 -0.7 z M50.31999999999999 14.120000000000001 l4.22 -6.28 c0.22 -0.32 0.58 -0.5 0.94 -0.5 c0.54 0 1.08 0.38 1.08 1.08 l0 6.16 l1.46 0 c0.44 0 0.66 0.34 0.66 0.68 s-0.22 0.66 -0.66 0.66 l-1.46 0 l0 2.84 c0 0.42 -0.34 0.64 -0.68 0.64 s-0.66 -0.22 -0.66 -0.64 l0 -2.84 l-3.92 0 c-1.04 0 -1.5 -1.02 -0.98 -1.8 z M51.599999999999994 14.58 l3.62 0 c0.02 -1.92 0.2 -3.8 0.22 -5.72 c-0.04 0.06 -0.06 0.12 -3.84 5.72 z M62.199999999999996 7.359999999999999 l6.04 0 c0.42 0 0.62 0.34 0.62 0.68 s-0.2 0.66 -0.62 0.66 l-2.28 0 l0 10.06 c0 0.42 -0.34 0.64 -0.68 0.64 s-0.66 -0.22 -0.66 -0.64 l0 -10.06 l-2.42 0 c-0.42 0 -0.62 -0.32 -0.62 -0.66 s0.2 -0.68 0.62 -0.68 z M72.9 15.86 l0 -4.96 c0 -2.4 1.24 -3.68 3.64 -3.68 l0.52 0 c2.38 0 3.64 1.28 3.64 3.68 l0 4.96 c0 2.2 -1.46 3.64 -3.64 3.64 l-0.52 0 c-2.22 0 -3.64 -1.46 -3.64 -3.64 z M76.54 18.16 l0.52 0 c1.44 0 2.3 -0.9 2.3 -2.3 l0 -4.96 c0 -1.56 -0.76 -2.36 -2.3 -2.36 l-0.52 0 c-1.52 0 -2.36 0.82 -2.36 2.36 l0 4.96 c0 1.46 0.92 2.3 2.36 2.3 z M94.5 18.76 l0 -8.92 l-3.18 9 c-0.18 0.5 -0.84 0.62 -1.04 0 l-3.08 -9 l0 8.92 c0 0.42 -0.32 0.64 -0.66 0.64 s-0.68 -0.22 -0.68 -0.64 l0 -10.74 c0 -0.38 0.28 -0.66 0.68 -0.66 l0.64 0 c0.44 0 0.68 0.32 0.78 0.62 c0.96 2.86 1.84 5.4 2.82 8.28 l2.9 -8.34 c0.1 -0.28 0.28 -0.56 0.7 -0.56 l0.8 0 c0.58 0 0.66 0.32 0.66 0.64 l0 10.76 c0 0.42 -0.34 0.64 -0.68 0.64 s-0.66 -0.22 -0.66 -0.64 z ">
          </path>
          </g>
          </svg>
        <!-- End Logo-->
        <!--
        <p style="color:white; margin-top:10px;">
          A new approach to topic modelling!
        </p>
        -->
        </div>
      </div>
      <div class="progress-container">
        <div class="progress-bar" id="myBar"></div>
      </div>
    </div>
    </div>
    <!-- End logo and logo background -->
    <div class="content" id="contents">
      <div id="toc"></div>

      <!-- Start of content-->
      <div style="padding-top:80px;">

        <h1>Installation</h1>
        <div class="subcontent">
          <p>
            To install code4tom you can use two methods, which you can find below.
          </p>
        </div>

        <h2 >Using Pip</h2>
        <div class="subcontent">
          <p>
            Some explainatory text: Lorem ipsum dolor sit amet, co
          </p>
          <pre><code>$> pip install code4tom</code></pre>
        </div>

        <h2>Using Github</h2>
        <div class="subcontent">
          <p>
            Some explainatory text: Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolore
          </p>
          <button class="mybutton" role="button" onclick=" window.open('https://github.com/wallnera/code4tom.git', '_blank'); return false;"><span class="text">Download</span></button>
        </div>

        <h1>Advantages and Disadvantages</h1>
        <div class="subcontent">
          <p>
            Studies assume that about 80% of the data on the Internet is available in text format. In addition to the professionally written texts on websites, vast amounts of generated content are posted daily on social media platforms. Whether on Twitter, Instagram, Reddit, or Facebook - the freely accessible texts of users on the Internet hold a true treasure to learn more about consumer behavior and the userÂ´s preferences, attitudes, and opinions. Unfortunately, texts tend to be unstructured and disorganized, making it difficult to analyze large amounts of text and gain meaningful insights quickly.
Topic modeling approaches try to structure texts by identifying common semantic features. Documents can thus be grouped into specific units about which thematic statements can be made. In recent years, numerous topic modeling approaches have been developed. In addition to Latent Dirichlet Allocation (LDA), which is probably the most widely used approch, algorithms such as LSA, STA, NMF, CorEx, BERTopic, and Top2Vec have been developed. Each of these algorithms has its advantages and disadvantages. Table 1 gives an overview of the advantages and disadvantages of the different approaches.
          </p>

          <h3 style="padding-top:20px;">LDA</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                  <li>Good in finding coherent topics.</li>
                	<li>Able to deal with sparse input. </li>
                	<li>LDA-learned features are easy to be interpreted</li>
                  <li>No prior domain-knowledge required</li>
                	<li>Mixed membership (one document can contain several topics)</li>
                	<li>Provides full generative models with multinominal distribution over topics</li>
                	<li>Shows adjectives and nouns in topics</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                  <li>Requires detailed assumptions and careful specification of hyperparameters </li>
                	<li>Topics are soft clusters which can result in overlapping topics </li>
                	<li>No objective evaluation metric available </li>
                	<li>Hard to find optimum number of topics (Number of topics needs to be specified by users)</li>
                	<li>Results are not deterministic, and reliability and validity cannot be taken for granted </li>
                	<li>Assumes that topics are independent of each other (using word co-appearance frequency only; Word correlations are disregarded, relations between topics cannot be modelled)</li>
                </ul>
              </td>
            </tr>
          </table>

          <h3 style="padding-top:20px;">NMF</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                  <li>Mixed membership (one document can contain several topics)</li>
                	<li>Term-document matrix with tf-idf weighting can be used instead of raw word frequencies like in LDA </li>
                	<li>Computationally efficient and highly scalable </li>
                	<li>Easy to be implemented </li>
                	<li>No prior domain knowledge required</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                  <li>Tends to provide incoherent topics</li>
                	<li>Hard to find optimum number of topics (Number of topics needs to be specified by users)</li>
                	<li>Implicit specification of probabilistic generative models </li>
                </ul>
              </td>
            </tr>
          </table>

          <h3 style="padding-top:20px;">CorEx</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                  <li>Does not require detailed assumptions and an underlying generative model </li>
                	<li>Has a good way to find optimum number of topics by increasing them until the correlation does not raise any more </li>
                	<li>Domain knowledge can be integrated through anchor words</li>
                	<li>Computational advantage for large datasets</li>
                	<li>Mixed membership (one document can contain several topics)</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                	<li>Not suitable to process longer texts; shorter subdocuments are recommended</li>
                	<li>Uninformative seed-words need to be added to obtain more intuitive topics </li>
                	<li>Each word is assigned to only one topic </li>
                </ul>
              </td>
            </tr>
          </table>


          <h3 style="padding-top:20px;">Top2Vec</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                  <li>Supports hierarchical topic reduction</li>
                	<li>Allows multilanguage analysis</li>
                	<li>Automatically finds the number of topics</li>
                	<li>Create jointly embedded word, document, and topic vectors</li>
                	<li>Search functions are built-in (easy to go from topic to documents, search topics, etc.)</li>
                	<li>Can work on very large dataset sizes</li>
                	<li>As it uses embeddings, no preprocessing of the original data is needed.</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                	<li>The embedding approach might result in too many topics, requiring labor-intensive inspection of each topic.</li>
                	<li>Generates many outliers</li>
                	<li>Not very suitable for small data set (< 1000)</li>
                	<li>Each document is assigned to one topic</li>
                	<li>Objective evaluation metrics are missing</li>
                </ul>
              </td>
            </tr>
          </table>

          <h3 id ="one" style="padding-top:20px;">BERTopic</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                <li>	High versatility and stability across domains</li>
                <li>	Allows multilanguage analysis</li>
                <li>	Supports topic modeling variations such as guided topic modeling, dynamic topic modeling or class-based topic modeling.</li>
                <li>	As it uses embeddings, no preprocessing of the original data is needed.</li>
                <li>	Automatically finds the number of topics</li>
                <li>	Supports hierarchical topic reduction</li>
                <li>	Search functions are built-in (easy to go from topic to documents, search topics, etc.)</li>
                <li>	Broader support of embedding models than Top2Vec</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                <li>	The embedding approach might result in too many topics, requiring labor-intensive inspection of each topic.</li>
                <li>	Generates many outliers</li>
                <li>	No topic distributions are generated within a single document. Each document is assigned to a single topic. </li>
                <li>	Objective evaluation metrics are missing</li>
                </ul>
              </td>
            </tr>
          </table>
          <p>
            Source: Egger & Yu (2021); Egger & Yu (2022)
          </p>

          <p style="padding-top:15px;">
            Code4Tom stands for âCommunity Detection for Topic Modelingâ and was developed to combine the advantages of the different topic modeling algorithms and reduce the respective disadvantages if possible.
          </p>

          <h3 style="padding-top:20px;" class="gradient-text">CODE4TOM</h3>
          <table width=100%>
            <tr>
              <td width=50% style="text-align:center;">
                <p style="font-size:20px;">Advantages</p>
              </td>
              <td width=50% style="text-align:center">
                <p style="font-size:20px;">Disadvantages</p>
              </td>
            </tr>
            <tr>
              <td style="vertical-align:top;">
                <ul>
                <li>	Prior domain knowledge is not necessarily required</li>
                <li>	High versatility and stability across domains</li>
                <li>	Allows for multilingual analysis</li>
                <li>	Uses embeddings so little to no preprocessing of the original data is needed</li>
                <li>	Automatically finds the number of topics</li>
                <li>	Any language model can be integrated</li>
                <li>	Modularity score indicates the quality of the clustering and serves as an objective evaluation metric</li>
                <li>	Outlayers are grouped together</li>
                <li>	Number of topics usually in an acceptable range to perform an analysis without the need to agglomerate clusters</li>
                <li>	Supports mixed membership models; thus, one document can contain several topics</li>
                <li>	Allows to select between POS and Keyword Extraction</li>
                <li>	Works on large and small datasets</li>
                <li>	Supports Topic labelling with GTP3</li>
                </ul>
              </td>
              <td style="vertical-align:top;">
                <ul>
                <li>No guided/seeded topic modeling up to now</li>
                </ul>
              </td>
            </tr>
          </table>
        </div>


        <h1>The Intuition</h1>
        <div class="subcontent">
          <p>
CODE4TOM tries to take advantage of the different approaches. In a first step, the document can optionally be split into individual sentences before minimal preprocessing is performed. For longer texts like reviews, it is recommended to split them into sentences. Concise texts like tweets or texts of Instagram posts should be treated as one document. During preprocessing, stop words are removed, lowercase is applied, numbers and special characters are removed, and tokenization is performed.
After that, keyword extraction is performed. There are two different approaches depending on the type of text and the objective. First, Yake! can be used for keyword extraction, or a POS is performed to use only certain types of words such as nouns, verbs, or adjectives.
After that, a document embedding is performed. Any language model can be used for this, but FastText is used by default. The document embedding vectorizes the extracted keywords of a unit (a document or a sentence), turning text into numbers.
Next, a community detection procedure (Louvain Clustering) is performed to find clusters in the high-dimensional data. A network is created, and communities are found in an iterative process by the algorithm. Topic Models such as Top2Vec and BERTopic also perform text embedding but require prior dimensionality reduction because density-based methods do not work well on high-dimensional data. Experience has shown that Louvain is very good at clustering high-dimensional data well. CODE4TOM, therefore, uses dimensionality reduction techniques such as t-SNE and UMAP for visualization purposes only. A problem with almost all topic modeling techniques is that they are unsupervised techniques without good evaluation metrics. In the case of CODE4TOM, the quality of clustering can be measured by the modularity of the network. The modularity score can range from -1 to ideally as close to 1 as possible.
While algorithms such as LDA and NMF support mixed-membership topics (a document can contain multiple topics), vector-based approaches such as Top2Vec and BERTopic do not. CODE4Tom also supports a mixed-membership, which is vital for a consistent and reliable data interpretation.
Basically, all topic modeling approaches have the problem of finding a suitable number of topics. While classical approaches require the a priori definition of the number of topics to be extracted, i.e. the number of topics must be specified by the researcher, vector-based models such as Top2Vec and BERTopic often provide a far too large number of topics. These approaches offer the possibility of downstream hierarchical agglomeration to reduce the often hundreds of topics, but at the latest, the researcher's specification is necessary again. The community-detection approach used by CODE4TOM will, in most cases, deliver between 10 and 25 topics, which is an optimal number to examine and describe the topics in detail later on.
Although topic modeling algorithms are mathematical-computational procedures, they are quantitative research methods. Although the extraction of the topics is done by the computer, the interpretation and meaning of the results are left to the researcher. CODE4TOM therefore offers a wide range of visualizations and possibilities to examine and interpret the data qualitatively.
In order to be able to judge the quality of the community detection, the graph created can be displayed together with the communities/topics found and the modularity score.
          </p>
        </div>

        <h1>Quick Guide</h1>
        <div class="subcontent">
          <p>
            Start with loading your data using pandas. Next, rename the column which you want to work with as âTodoâ as shown in the example below.
          </p>
          <pre><code class="python">df = pd.read_csv("data/covidtweets.csv", sep=";")
name_of_text_row = "text"
df = df[[name_of_text_row]]
df["Todo"] = df[name_of_text_row].astype(str)</code></pre>

          <p style="padding-top:10px;">
            Continue by tokenizing your data.
          </p>
          <pre><code class="python">df["tokens"] = tokenize(df)
# remove samples with small sizes
df = df[df['tokens'].map(lambda d: len(d)) > 2]
df = df.reset_index(drop=True)</code></pre>

<p style="padding-top:10px;">
  Choose if you want to use Yake! Keywords or POS tagging to obtain a list of words used for the FastText model.
</p>
<pre><code class="python">toplist = get_toplist(df,use_yake=True)</code></pre>

<p style="padding-top:10px;">
  Generate your model.
</p>
<pre><code class="python">model = generate_model(toplist)</code></pre>

<p style="padding-top:10px;">
  Calculate the row vectors.
</p>
<pre><code class="python">rowvectors = []
for item in toplist:
    rowvectors.append(documentembedding(item, model))

df["rowvectors"] = rowvectors

# furthermore, save rowvectors as a df_vec
df_vec = pd.DataFrame(rowvectors)
df_vec = df_vec.dropna()</code></pre>

<p style="padding-top:10px;">
  Before applying clustering to the data, normalize it using PCA.
</p>
<pre><code class="python">df_vec = normalize(df_vec, norm='l2')
pca = decomposition.PCA()
pca.n_components = 20
pca_vec = pca.fit_transform(df_vec)</code></pre>

<p style="padding-top:10px;">
  Having obtained PCA vectors, we can now use Louvain clustering on our data.
</p>
<pre><code class="python">p,c,G = cluster_louvain(pca_vec)</code></pre>

<p style="padding-top:10px;">
  To assess the goodness of fit, use modularity. This value should be >0.5, if this is not the case try different values.
</p>
<pre><code class="python">get_modularity(p,G)</code></pre>

<p style="padding-top:10px;">
Now you can start inspecting the clusters found by Louvain clustering.
To only use a few documents, use the function get_document_per_cluster. This function searches the cluster with the least amount of documents and returns a minimum amount of documents you are using for inspection.
</p>
<pre><code class="python">doc_per_cluster = get_document_per_cluster(df_vec, percentage=0.001, maximum_doc_size=10)</code></pre>

<p style="padding-top:10px;">
  Inspecting the clusters. (Here cluster with ID 3)
</p>
<pre><code class="python">inspect_cluster(df_vec, 3, doc_per_cluster)</code></pre>

<p style="padding-top:10px;">
  To get even more out of your cluster, check out GPT3 or GooseAI
</p>
<pre><code class="python">use_gpt3(df_vec, doc_per_cluster)</code></pre>
<pre><code class="python">use_goose(df_vec, doc_per_cluster)</code></pre>

        </div>
      </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js"></script>
    <script>hljs.highlightAll()</script>

    <script>
      // When the user scrolls the page, execute myFunction
      window.onscroll = function() {myFunction()};

      function myFunction() {
        var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
        var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        var scrolled = (winScroll / height) * 100;
        document.getElementById("myBar").style.width = scrolled + "%";
      }
    </script>

    <script>
    window.onload = function () {
    var toc = "";
    var level = 0;

    document.getElementById("contents").innerHTML =
        document.getElementById("contents").innerHTML.replace(
            /<h([\d])>([^<]+)<\/h([\d])>/gi,
            function (str, openLevel, titleText, closeLevel) {
                if (openLevel != closeLevel) {
                    return str;
                }

                if (openLevel > level) {
                    toc += (new Array(openLevel - level + 1)).join("<ul>");
                } else if (openLevel < level) {
                    toc += (new Array(level - openLevel + 1)).join("</ul>");
                }

                level = parseInt(openLevel);

                var anchor = titleText.replace(/ /g, "_");
                toc += "<li><a href=\"#" + anchor + "\">" + titleText
                    + "</a></li>";

                return "<h" + openLevel + "><a name=\"" + anchor + "\">"
                    + titleText + "</a></h" + closeLevel + ">";
            }
        );

    if (level) {
        toc += (new Array(level + 1)).join("</ul>");
    }

    document.getElementById("toc").innerHTML += toc;
};
    </script>
  </body>
</html>
